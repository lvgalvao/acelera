# Acelera - Seus melhores 90 dias

<div align="center">
  <img src="img/acelerador.jpg" alt="Acelerador - Jornada de Dados" width="600">
</div>

## ğŸ¯ **Receba seu roadmap personalizado em 2 minutos!**

**Simples assim:** Responda 7 perguntas e receba automaticamente seu **roadmap de estudo para os prÃ³ximos 90 dias**, personalizado para seu objetivo profissional.

### ğŸŒ **Acesse diretamente:**
**[https://roadmap-jornadadedados.streamlit.app//](https://lvgalvao-acelera-main-tplci3.streamlit.app/)**

### ğŸš€ **O que vocÃª vai receber:**
- âœ… **3 trilhas recomendadas** baseadas no seu perfil
- âœ… **Plano de estudos de 3 meses** (1 trilha por mÃªs)
- âœ… **Cronograma detalhado** com todos os mÃ³dulos
- âœ… **Download do plano** em CSV para acompanhar
- âœ… **CÃ¡lculo baseado em 2 horas de estudo por dia**

## ğŸ® **Como usar (super simples):**

1. **Execute a aplicaÃ§Ã£o** (instruÃ§Ãµes abaixo)
2. **Clique em "Fazer QuestionÃ¡rio"**
3. **Responda 7 perguntas** sobre seu perfil
4. **Receba seu roadmap** personalizado automaticamente!

---

*O programa Acelera Ã© uma iniciativa da [Jornada de Dados](https://suajornadadedados.com.br/) para acelerar sua carreira em dados.*

## ğŸ“š **Trilhas DisponÃ­veis:**
- **n8n** - AutomaÃ§Ã£o de processos
- **SQL** - Banco de dados e consultas  
- **Python** - ProgramaÃ§Ã£o para dados
- **Engenharia de Dados + IA** - Pipelines e infraestrutura
- **AWS** - Cloud computing
- **âš¡ Spark & Databricks** - Plataforma de dados moderna

## ğŸ¯ **Desafios TÃ©cnicos das Trilhas:**

### ğŸ **Python**
- [Desafio 01: ValidaÃ§Ã£o de Dados e CÃ¡lculo de BÃ´nus](desafios/python/desafio_01.md)
  - ValidaÃ§Ã£o de dados de funcionÃ¡rios
  - CÃ¡lculo de bÃ´nus com regras especÃ­ficas
  - GeraÃ§Ã£o de relatÃ³rios em CSV e JSON

### ğŸ—„ï¸ **SQL**
- [Desafio 01: FunÃ§Ãµes de Ranking (ROW_NUMBER, RANK e DENSE_RANK)](desafios/sql/desafio_01.md)
  - AnÃ¡lise de atividade de e-mails
  - ComparaÃ§Ã£o entre funÃ§Ãµes de ranking
  - Problema clÃ¡ssico de entrevistas tÃ©cnicas

### â˜ï¸ **AWS**
- [Desafio 01: Streamlit no EC2 com Kaggle + S3](desafios/aws/desafio_01.md)
  - Dashboard Streamlit em instÃ¢ncia EC2
  - IntegraÃ§Ã£o com datasets do Kaggle
  - Armazenamento e leitura de dados no S3

### ğŸ”§ **Engenharia de Dados**
- [Desafio 01: Docker + Airflow (ETL Bitcoin)](desafios/engenharia/desafio_01.md)
  - Pipeline ETL com Airflow em Docker
  - Coleta de dados da API Coinbase
  - Agendamento e persistÃªncia de dados

### ğŸ¤– **n8n**
- [Desafio 01: Agente Financeiro (Telegram + Google Sheets + ChatGPT)](desafios/n8n/desafio_01.md)
  - Bot Telegram para controle financeiro
  - IntegraÃ§Ã£o com Google Sheets
  - ClassificaÃ§Ã£o automÃ¡tica de intenÃ§Ãµes com IA

## ğŸ¯ **Perfis Atendidos:**
- ğŸ‘¨â€ğŸ’¼ **Profissional migrando para a Ã¡rea**
- ğŸ“Š **Analista migrando para engenharia**
- ğŸš€ **Engenheiro se especializando**

## ğŸ› ï¸ **Como usar:**

### ğŸŒ **OpÃ§Ã£o 1 - Acesse diretamente (Recomendado):**
**[https://lvgalvao-acelera-main-tplci3.streamlit.app/](https://lvgalvao-acelera-main-tplci3.streamlit.app/)**

### ğŸ’» **OpÃ§Ã£o 2 - Execute localmente:**

1. **Clone e instale:**
```bash
git clone https://github.com/lvgalvao/acelera.git
cd acelera
pip install -r requirements.txt
```

2. **Configure o banco de dados (opcional):**
```bash
# Copie o arquivo de exemplo
cp env_example.txt .env
# Edite o arquivo .env com suas credenciais do Supabase
```

3. **Execute:**
```bash
streamlit run main.py
```

4. **Acesse:** `http://localhost:8501`

> **ğŸ’¡ Dica:** Se vocÃª quiser salvar as respostas do questionÃ¡rio no banco de dados, configure o arquivo `.env` com suas credenciais do Supabase. Veja o guia completo em `POSTGRESQL_SETUP.md`.

---

## ğŸ¯ **Ã‰ isso! Simples assim:**

1. **Execute** a aplicaÃ§Ã£o
2. **Responda** 7 perguntas  
3. **Receba** seu roadmap personalizado
4. **Comece** seus 90 dias de transformaÃ§Ã£o!

**Pronto para acelerar sua carreira em dados?** ğŸš€

---

**Desenvolvido com â¤ï¸ para acelerar sua jornada em dados!**

*Este projeto faz parte da **Jornada de Dados** - uma iniciativa para democratizar o conhecimento em dados e acelerar a carreira de profissionais na Ã¡rea.*
