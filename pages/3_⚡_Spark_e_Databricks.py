import streamlit as st
import pandas as pd
import sys
import os

# Adicionar o diret√≥rio raiz ao path para importar m√≥dulos
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="Spark e Databricks - Jornada de Dados",
    page_icon="‚ö°",
    layout="wide"
)

# For√ßar tema light
st.markdown("""
<style>
    .stApp {
        color-scheme: light;
    }
    .stApp > header {
        background-color: transparent;
    }
    .stApp > div {
        background-color: #ffffff;
    }
    /* For√ßa o tema light em todos os componentes */
    .stSelectbox > div > div {
        background-color: #ffffff;
        color: #262730;
    }
    .stTextInput > div > div > input {
        background-color: #ffffff;
        color: #262730;
    }
    .stTextArea > div > div > textarea {
        background-color: #ffffff;
        color: #262730;
    }
    .stButton > button {
        background-color: #ffffff;
        color: #262730;
        border: 1px solid #cccccc;
    }
    .stButton > button:hover {
        background-color: #f0f2f6;
        border-color: #1f77b4;
    }
    .stDataFrame {
        background-color: #ffffff;
    }
    .stExpander {
        background-color: #ffffff;
        border: 1px solid #cccccc;
    }
    .stMetric {
        background-color: #ffffff;
    }
    .stInfo {
        background-color: #e6f3ff;
        border: 1px solid #1f77b4;
    }
    .stSuccess {
        background-color: #e6ffe6;
        border: 1px solid #28a745;
    }
    .stWarning {
        background-color: #fff3cd;
        border: 1px solid #ffc107;
    }
</style>
""", unsafe_allow_html=True)

# T√≠tulo principal
st.title("‚ö° Spark e Databricks")
st.markdown("**Plataforma de dados moderna com Spark e Databricks**")
st.info("üìÖ In√≠cio das turmas: 13, 14 e 15 de outubro")
st.markdown("---")

# Dados hardcoded do Spark e Databricks
@st.cache_data
def load_databricks_data():
    dados = [
        {"conteudo": "Introdu√ß√£o ao Databricks e ao Lakehouse", "carga_horaria": 90, "objetivo": "Compreender a vis√£o geral da plataforma e o conceito de Lakehouse.", "certificacao": "Associate ‚úÖ"},
        {"conteudo": "Arquitetura de Dados Moderna", "carga_horaria": 90, "objetivo": "Diferenciar Data Lake, Data Warehouse e Lakehouse.", "certificacao": "Associate ‚úÖ"},
        {"conteudo": "Configura√ß√£o do Workspace Databricks", "carga_horaria": 90, "objetivo": "Aprender a criar clusters, gerenciar usu√°rios e permiss√µes.", "certificacao": "Associate ‚úÖ"},
        {"conteudo": "Primeiro Notebook no Databricks", "carga_horaria": 90, "objetivo": "Executar PySpark e SQL, integrando notebooks com Git.", "certificacao": "Associate ‚úÖ"},
        {"conteudo": "Fundamentos do Spark e PySpark", "carga_horaria": 90, "objetivo": "Entender RDDs, DataFrames e paralelismo.", "certificacao": "Associate ‚úÖ"},
        {"conteudo": "Transforma√ß√µes e A√ß√µes no Spark", "carga_horaria": 90, "objetivo": "Praticar opera√ß√µes como filter, map, groupBy e join.", "certificacao": "Associate ‚úÖ"},
        {"conteudo": "Leitura e Escrita de Dados", "carga_horaria": 90, "objetivo": "Carregar e salvar dados em CSV, JSON, Parquet e Delta Lake.", "certificacao": "Associate ‚úÖ"},
        {"conteudo": "SQL no Databricks", "carga_horaria": 90, "objetivo": "Utilizar queries SQL, views e otimiza√ß√µes b√°sicas.", "certificacao": "Associate ‚úÖ"},
        {"conteudo": "Ingest√£o de Dados Batch", "carga_horaria": 90, "objetivo": "Integrar dados de APIs, bancos relacionais e arquivos externos.", "certificacao": "Associate ‚úÖ"},
        {"conteudo": "Camadas Bronze, Silver e Gold", "carga_horaria": 90, "objetivo": "Implementar pipelines estruturadas no modelo de camadas.", "certificacao": "Associate ‚úÖ"},
        {"conteudo": "Qualidade e Governan√ßa de Dados", "carga_horaria": 90, "objetivo": "Aplicar schema enforcement e data quality checks.", "certificacao": "Associate ‚úÖ"},
        {"conteudo": "Delta Lake Avan√ßado", "carga_horaria": 90, "objetivo": "Explorar ACID transactions, time travel e otimiza√ß√µes.", "certificacao": "Associate ‚úÖ / Professional ‚≠ê"},
        {"conteudo": "Conceitos de Streaming no Spark", "carga_horaria": 90, "objetivo": "Entender microbatch vs continuous e arquitetura de streaming.", "certificacao": "Associate ‚úÖ"},
        {"conteudo": "Structured Streaming com PySpark", "carga_horaria": 90, "objetivo": "Ler dados de Kafka e sockets em tempo real.", "certificacao": "Associate ‚úÖ / Professional ‚≠ê"},
        {"conteudo": "Transforma√ß√µes e Janelas em Streaming", "carga_horaria": 90, "objetivo": "Realizar agrega√ß√µes e aplicar watermarks em fluxos.", "certificacao": "Professional ‚≠ê"},
        {"conteudo": "Integra√ß√£o com Dashboards em Tempo Real", "carga_horaria": 90, "objetivo": "Conectar dados em streaming ao Power BI e Databricks SQL.", "certificacao": "Associate ‚úÖ"},
        {"conteudo": "Orquestra√ß√£o com Databricks Workflows", "carga_horaria": 90, "objetivo": "Configurar jobs, triggers e depend√™ncias.", "certificacao": "Associate ‚úÖ / Professional ‚≠ê"},
        {"conteudo": "Integra√ß√£o com dbt-core e SQL", "carga_horaria": 90, "objetivo": "Criar modelos versionados e documentados.", "certificacao": "Professional ‚≠ê"},
        {"conteudo": "Infraestrutura como C√≥digo no Databricks", "carga_horaria": 90, "objetivo": "Automatizar clusters e permiss√µes com Terraform.", "certificacao": "Professional ‚≠ê"},
        {"conteudo": "Projeto Batch + Streaming", "carga_horaria": 90, "objetivo": "Construir pipeline real unindo ingest√£o batch e streaming.", "certificacao": "Associate ‚úÖ / Professional ‚≠ê"},
        {"conteudo": "Otimiza√ß√£o de Jobs Spark", "carga_horaria": 90, "objetivo": "Aplicar t√©cnicas de particionamento, caching e adaptive execution.", "certificacao": "Professional ‚≠ê"},
        {"conteudo": "Seguran√ßa e Governan√ßa com Unity Catalog", "carga_horaria": 90, "objetivo": "Configurar permiss√µes, lineage e auditoria.", "certificacao": "Professional ‚≠ê"},
        {"conteudo": "Prepara√ß√£o para Certifica√ß√£o Databricks", "carga_horaria": 90, "objetivo": "Revisar conte√∫do oficial e realizar simulados.", "certificacao": "Associate ‚úÖ / Professional ‚≠ê"},
        {"conteudo": "Projeto Final End-to-End", "carga_horaria": 90, "objetivo": "Desenvolver pipeline completo com batch, streaming, dashboards e governan√ßa.", "certificacao": "Professional ‚≠ê"},
    ]
    
    return pd.DataFrame(dados)

def main():
    # Carregar dados
    df_databricks = load_databricks_data()
    
    # Informa√ß√µes gerais da trilha
    st.markdown("## üìä Vis√£o Geral da Trilha")
    
    # Calcular estat√≠sticas (convertendo minutos para horas)
    total_modulos = len(df_databricks)
    total_horas = df_databricks['carga_horaria'].sum() / 60.0
    total_dias = total_horas / 2  # Considerando 2 horas por dia
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("üìö Total de M√≥dulos", total_modulos)
    
    with col2:
        st.metric("‚è±Ô∏è Carga Hor√°ria Total", f"{total_horas:.1f}h")
    
    with col3:
        st.metric("üìÖ Dias de Estudo", f"{total_dias:.0f} dias")
    
    with col4:
        st.metric("üìÜ Dura√ß√£o Estimada", f"{total_dias/30:.1f} meses")
    
    st.markdown("---")
    
    # Filtros e visualiza√ß√µes
    st.markdown("## üîç Explorar M√≥dulos")
    
    # Filtro por certifica√ß√£o
    certificacoes = df_databricks['certificacao'].unique()
    certificacao_selecionada = st.selectbox(
        "Filtrar por Certifica√ß√£o:",
        options=["Todas"] + list(certificacoes),
        help="Selecione uma certifica√ß√£o para filtrar os m√≥dulos"
    )
    
    # Aplicar filtro
    if certificacao_selecionada != "Todas":
        df_filtrado = df_databricks[df_databricks['certificacao'] == certificacao_selecionada]
    else:
        df_filtrado = df_databricks
    
    # Mostrar estat√≠sticas do filtro
    if certificacao_selecionada != "Todas":
        st.info(f"üìã Mostrando {len(df_filtrado)} m√≥dulos para certifica√ß√£o: **{certificacao_selecionada}**")
    
    # Tabela dos m√≥dulos
    st.markdown("### üìö M√≥dulos da Trilha")
    
    # Preparar dataframe para exibi√ß√£o
    df_display = pd.DataFrame({
        'M√≥dulo': df_filtrado['conteudo'],
        'Carga Hor√°ria (h)': df_filtrado['carga_horaria'] / 60.0,
        'Dias Necess√°rios': (df_filtrado['carga_horaria'] / 60.0 / 2.0).round(1),
        'Objetivo': df_filtrado['objetivo'],
        'Certifica√ß√£o': df_filtrado['certificacao'],
    })
    
    st.dataframe(
        df_display,
        height=400,
        use_container_width=True
    )
    
    # Bot√£o para download
    csv_data = df_display.to_csv(index=False)
    st.download_button(
        label="üì• Baixar M√≥dulos (CSV)",
        data=csv_data,
        file_name="modulos_spark_databricks.csv",
        mime="text/csv",
        type="primary"
    )
    
    st.markdown("---")
    
    # An√°lise por certifica√ß√£o
    st.markdown("## üèÜ An√°lise por Certifica√ß√£o")
    
    # Contar m√≥dulos por certifica√ß√£o
    certificacao_counts = df_databricks['certificacao'].value_counts()
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### üìä Distribui√ß√£o de M√≥dulos")
        st.bar_chart(certificacao_counts)
    
    with col2:
        st.markdown("### üìà Estat√≠sticas por Certifica√ß√£o")
        
        for cert, count in certificacao_counts.items():
            modulos_cert = df_databricks[df_databricks['certificacao'] == cert]
            horas_cert = modulos_cert['carga_horaria'].sum() / 60.0
            dias_cert = horas_cert / 2
            
            with st.expander(f"üèÜ {cert} ({count} m√≥dulos)"):
                st.metric("M√≥dulos", count)
                st.metric("Horas", f"{horas_cert:.1f}h")
                st.metric("Dias", f"{dias_cert:.0f} dias")
    
    st.markdown("---")
    
    # Roadmap de estudo
    st.markdown("## üó∫Ô∏è Roadmap de Estudo Sugerido")
    
    # Dividir em fases baseado na certifica√ß√£o
    associate_modules = df_databricks[df_databricks['certificacao'].str.contains('Associate', na=False)]
    professional_modules = df_databricks[df_databricks['certificacao'].str.contains('Professional', na=False)]
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### ü•â Fase 1: Associate (Fundamentos)")
        st.markdown(f"**{len(associate_modules)} m√≥dulos - {associate_modules['carga_horaria'].sum()/60.0/2:.0f} dias de estudo**")
        
        with st.expander("Ver m√≥dulos Associate", expanded=True):
            for idx, row in associate_modules.iterrows():
                st.markdown(f"**{row['conteudo']}**")
                st.markdown(f"‚è±Ô∏è {row['carga_horaria']/60.0:.1f}h | üéØ {row['objetivo']}")
                st.markdown("---")
    
    with col2:
        st.markdown("### ü•á Fase 2: Professional (Avan√ßado)")
        st.markdown(f"**{len(professional_modules)} m√≥dulos - {professional_modules['carga_horaria'].sum()/60.0/2:.0f} dias de estudo**")
        
        with st.expander("Ver m√≥dulos Professional", expanded=True):
            for idx, row in professional_modules.iterrows():
                st.markdown(f"**{row['conteudo']}**")
                st.markdown(f"‚è±Ô∏è {row['carga_horaria']/60.0:.1f}h | üéØ {row['objetivo']}")
                st.markdown("---")
    
    st.markdown("---")
    
    # Dicas e recursos
    st.markdown("## üí° Dicas e Recursos")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("### üéØ **Foco Associate**")
        st.markdown("""
        - Domine os fundamentos do Spark
        - Pratique com notebooks Databricks
        - Entenda o conceito de Lakehouse
        - Trabalhe com Delta Lake b√°sico
        """)
    
    with col2:
        st.markdown("### üöÄ **Foco Professional**")
        st.markdown("""
        - Aprofunde em streaming
        - Domine Unity Catalog
        - Automatize com Terraform
        - Otimize performance
        """)
    
    with col3:
        st.markdown("### üìö **Recursos √öteis**")
        st.markdown("""
        - [Documenta√ß√£o Databricks](https://docs.databricks.com/)
        - [Spark Documentation](https://spark.apache.org/docs/)
        - [Delta Lake Guide](https://delta.io/)
        - [Databricks Academy](https://academy.databricks.com/)
        """)
    
    st.markdown("---")
    
    # Footer
    st.markdown("""
    <div style="text-align: center; color: #666; font-size: 0.9em;">
        <p>‚ö° Spark e Databricks - Transformando dados em insights</p>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
